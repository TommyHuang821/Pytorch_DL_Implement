{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exposed-hands",
   "metadata": {},
   "source": [
    "# 前一段訓練程式看到衍伸函數\n",
    "1. optimizer.zero_grad()<br>\n",
    "2. with torch.no_grad()<br>\n",
    "\n",
    "這些功能在幹嘛? (見下範例程式碼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.train()\n",
    "for batch_idx, (data, target) in enumerate(dataloader_train):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # MLP\n",
    "    optimizer_mlp.zero_grad()\n",
    "    output_mlp = model_mlp(data)\n",
    "    loss_mlp=loss(output_mlp,target)        \n",
    "    loss_mlp.backward()\n",
    "    optimizer_mlp.step()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for data, target in dataloader_test:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "         # MLP\n",
    "        output_mlp = model_mlp(data)\n",
    "        test_loss_mlp += loss(output_mlp, target)\n",
    "        pred_mlp = output_mlp.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct_mlp += pred_mlp.eq(target.view_as(pred_mlp)).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-confirmation",
   "metadata": {},
   "source": [
    "## 1. optimizer.zero_grad()\n",
    "<font size=4 color=red> **此函數是用在將梯度歸0。** </font><br>\n",
    "\n",
    "<font size=4 color=red> **- 什麼叫梯度歸0?** </font>\n",
    "\n",
    "<font size=3 > 在每個batch在模型inference中，模型會將梯度同時也算出來，也就是此段程式碼 (output = model(data))\n",
    "\n",
    "如果在pytorch不使用zero_grad()，也就是每次模型更新不歸0會發生什麼事情<br>\n",
    "    \n",
    "範例:  $y=x^2 → \\frac{{\\rm d}y}{{\\rm d}x}=\\frac{{\\rm d}x^2 }{{\\rm d}x}\\ = 2x $<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lesbian-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n",
      "x:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第1次梯度:tensor([2., 4., 6.], dtype=torch.float64)\n",
      "x:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第2次梯度:tensor([ 4.,  8., 12.], dtype=torch.float64)\n",
      "x:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第3次梯度:tensor([ 6., 12., 18.], dtype=torch.float64)\n",
      "x:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第4次梯度:tensor([ 8., 16., 24.], dtype=torch.float64)\n",
      "x:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第5次梯度:tensor([10., 20., 30.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "x = np.array([1.0,2.0,3.0])\n",
    "x = torch.tensor(x, requires_grad=True)\n",
    "print('x:{}'.format(x))\n",
    "for i in range(5): \n",
    "    print('x:{}'.format(x.data))\n",
    "    y = x**2\n",
    "    y.sum().backward()\n",
    "    # y.mean().backward()\n",
    "    print('第{}次梯度:{}'.format(i+1, x.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-messenger",
   "metadata": {},
   "source": [
    "<font size=3> \n",
    "    所以可以發現如果我們沒有清掉梯度，在backward()部分梯度會將舊的梯度一直累加上去<br>\n",
    "    所以需要將參數自行歸0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "horizontal-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "input:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第1次梯度:tensor([2., 4., 6.], dtype=torch.float64)\n",
      "**********\n",
      "input:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第2次梯度:tensor([2., 4., 6.], dtype=torch.float64)\n",
      "**********\n",
      "input:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第3次梯度:tensor([2., 4., 6.], dtype=torch.float64)\n",
      "**********\n",
      "input:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第4次梯度:tensor([2., 4., 6.], dtype=torch.float64)\n",
      "**********\n",
      "input:tensor([1., 2., 3.], dtype=torch.float64)\n",
      "第5次梯度:tensor([2., 4., 6.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# simple gradient\n",
    "a = np.array([1.0, 2.0, 3.0])\n",
    "a = torch.tensor(a, requires_grad=True)\n",
    "for i in range(5): \n",
    "    c = a * a \n",
    "    out = c.sum()\n",
    "    out.backward()\n",
    "    print('*'*10)\n",
    "    print('input:{}'.format(a.data))\n",
    "    print('第{}次梯度:{}'.format(i+1, a.grad.data))\n",
    "    a.grad.detach_()\n",
    "    a.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-princess",
   "metadata": {},
   "source": [
    " ## <font color=blue> 梯度為歸0手寫範例\n",
    "<font size=3>**梯度下降法**\n",
    "<font size=3> \\begin{gather*}\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\times f'(\\theta^{(t)})\n",
    "\\end{gather*}\n",
    "<font size=3> $f'(\\theta)$為梯度，$\\eta$為學習率(Learning rate)<br>\n",
    "假設初始的$\\theta^{(0)}$為10 <br>\n",
    "\n",
    "1. 第一個batch的梯度$f'(\\theta^{(0)})$為 2，$\\eta$為1<br>\n",
    "$$\n",
    "\\theta^{(1)} = \\theta^{(0)} - \\eta \\times f'(\\theta^{(0)}) = 10 - 1 \\times 2 = 8\n",
    "$$\n",
    "\n",
    "  \n",
    "2. 第二個batch的梯度$f'(\\theta^{(1}))$為 1 <br>\n",
    "如果梯度有歸0，則梯度更新則是\n",
    "$$\n",
    "\\theta^{(2)} = \\theta^{(1)} - \\eta \\times \\color{red}{f'(\\theta^{(1)})} = 8 - 1 \\times 1 = 7\n",
    "$$ \n",
    "如果梯度沒有歸0，則梯度會累加，更新方式則是\n",
    "$$\n",
    "\\theta^{(2)} = \\theta^{(1)} - \\eta \\times \\color{red}{(f'(\\theta^{(1)})+f'(\\theta^{(0)}))} = 7 - 1 \\times (1+2) = 4\n",
    "$$\n",
    "    \n",
    "3. 第三個batch的梯度$f'(\\theta^{(1}))$為 -1 <br>\n",
    "如果梯度有歸0，則梯度更新則是\n",
    "$$    \n",
    "\\theta^{(3)} = \\theta^{(2)} - \\eta \\times \\color{red}{f'(\\theta^{(2)})} = 7 - 1 \\times (-1) = 8\n",
    "$$\n",
    "如果梯度沒有歸0，則梯度會累加，更新方式則是\n",
    "$$\n",
    "\\theta^{(3)} = \\theta^{(2)} - \\eta \\times \\color{red}{(f'(\\theta^{(2)})+f'(\\theta^{(1)})+f'(\\theta^{(0)}))} = 4 - 1 \\times (1+2-1) = 2\n",
    "$$     \n",
    "</font>   \n",
    "\n",
    "Note:梯度不歸零不一定是用累積的方式，也能用平均的方式進行梯度融合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
