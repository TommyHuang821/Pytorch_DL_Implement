{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of Classifier Cat and Dog Model\n",
    "<br />\n",
    "<font size=3>\n",
    "此篇範例用pytorch的CNN模型做貓跟狗的分類，並且做模型量化。一般在做量化模型程式工具，為了可以套用各種模型，程式會寫許多API來支援各種案例，但為了方便解釋每個量化過程，所以程式寫法會比較不精簡化，如果有興趣的話可以在自行研究如何API化。<br />\n",
    "<br />\n",
    "這裡稍微敘述一下，在整個模型做量化的實際過程:<br />\n",
    "<br />\n",
    "1. 建立模型，並且在模型中建立觀測數據的節點:<br />\n",
    "&emsp;&emsp;通常我們會在參數型layer(ex. CONV、BN、FC)的輸出處植入觀測數據範圍的功能，之後才可以計算這些layer的量化參數scale。<br />\n",
    "<br />\n",
    "2. Inference calibration dataset:<br />\n",
    "&emsp;&emsp;在量化時，會先準備一筆dataset跑一遍模型，讓事先埋入觀測數據範圍的函數紀錄data range。<br />\n",
    "<br />\n",
    "3. 開始量化:<br />\n",
    "&emsp;&emsp;a. 在量化前，可以先把CONV和BN的weight和bias先融合在一起，當然這步驟也可以在跑inference calibration前就先做。<br />\n",
    "&emsp;&emsp;b. 將每層的輸出scale都計算好，再來計算weight和bias的scale，把整個模型都量化好。<br />\n",
    "&emsp;&emsp;c. 模型量化好後，就可以執行量化的inference做測試了。<br />\n",
    "\n",
    "<br />\n",
    "可以開始寫code了，若有GPU的電腦就可以使用CUDA執行，沒有的話就使用CPU，速度不會差很多，因為這邊並沒有要做訓練。<br />\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchnet.meter as tnt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN model with quantization function<br />\n",
    "<br />\n",
    "<font size=3>\n",
    "接下來看模型部分會有些複雜，但一個個仔細解釋就會變得簡單許多，先分別每個功能一個個看。<br />\n",
    "</font>\n",
    "<br />\n",
    "\n",
    "### def __init__(self):<br />\n",
    "<font size=3>\n",
    "先來看一下我們要使用的模型結構，這裡使用了4層的CONV和兩層FC，每層CONV都會接著BN、ReLU和MaxPool，而FC會接ReLU，通常最後一層就不會再接ReLU，以上是基本模型的結構。<br />\n",
    "再來是量化的功能，我們在輸入處和每一層ReLU輸出後面都加了scale和range這兩個參數，當然也可以放在MaxPool層後面，其實效果不會差太多，range參數會再做inference calibration的時候儲存此資料範圍，而scale會在量化模型的時候計算。<br />\n",
    "</font>\n",
    "<br />\n",
    "\n",
    "### def update_range(self, data, range_data):<br />\n",
    "<font size=3>\n",
    "這個函數很好理解，就是不斷更新此層的資料範圍，如果目前的batch data比過去的範圍還大，就更新參數。<br />\n",
    "</font>\n",
    "<br />\n",
    "\n",
    "### quantize(self, x, scale, int_range):<br />\n",
    "<font size=3>\n",
    "顧名思義，就是將數據量化，比較需要特別說明的是torch.clip(xq, -int_range, int_range)這段函式，它用來確保量化後的值能夠在int8資料型態內，也就是資料範圍會限制在-127~128。<br />\n",
    "</font>\n",
    "<br />\n",
    "\n",
    "### weight_quant(self, w, b, in_scale):<br />\n",
    "<font size=3>\n",
    "這裡就跟之前講過的量化式一樣的，比較需要注意的時bias的量化參數scale是由weight和input的scale計算出來的，原因就不在這贅述。<br />\n",
    "</font>\n",
    "<br />\n",
    "\n",
    "### fusion_conv_bn_quant(self, conv, bn, in_scale):<br />\n",
    "<font size=3>\n",
    "融合CONV和BN這兩個layer，這也跟先前的範例一樣。<br />\n",
    "</font>\n",
    "<br />\n",
    "\n",
    "### cl_scale(self):<br />\n",
    "<font size=3>\n",
    "在inference calibration後，就可以使用這個函數開始計算每層layer的feature map和weight的scale，並且把模型內的weight和bias做量化，模型將由FP32轉成INT8。<br />\n",
    "</font>\n",
    "<br />\n",
    "\n",
    "### forward(self, x):<br />\n",
    "<font size=3>\n",
    "forward這裡切成兩種模式(FP32 and INT8)，FP32模式可以看到在每層ReLU的輸出處加入觀測數據範圍的函數，而INT8則是在模型輸入層和每個ReUL層後面都加入量化層。<br />\n",
    "輸入層的量化很好理解，而ReLU後的量化在這解釋一下，因為經過參數層(CONV、BN、FC)的運算，此時的feature map已經包含input and weight的scale，這時候的值域範圍還不是原本輸出該有的值域範圍，資料型態範圍也不會是INT8，而會是INT32，為了將feature map量化到輸出值域範圍，會先還原input and weight的scale，在用此層的scale量化到該值域。\n",
    "</font>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        # Convolution 1 , input_shape=(3,224,224)\n",
    "        self.scale_input = torch.tensor(1.0) # quant-scale\n",
    "        self.range_input = torch.tensor(1.0) # quant-range\n",
    "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.scale_cv1 = torch.tensor(1.0) # quant-scale\n",
    "        self.range_cv1 = torch.tensor(1.0) # quant-range\n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.scale_cv2 = torch.tensor(1.0) # quant-scale\n",
    "        self.range_cv2 = torch.tensor(1.0) # quant-range\n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        # Convolution 3\n",
    "        self.cnn3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.scale_cv3 = torch.tensor(1.0) # quant-scale\n",
    "        self.range_cv3 = torch.tensor(1.0) # quant-range\n",
    "        # Max pool 3\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        # Convolution 4\n",
    "        self.cnn4 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(8)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.scale_cv4 = torch.tensor(1.0) # quant-scale\n",
    "        self.range_cv4 = torch.tensor(1.0) # quant-range\n",
    "        # Max pool 4\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        # Fully connected 1 ,#input_shape=(8*12*12)\n",
    "        self.fc1 = nn.Linear(8 * 11 * 11, 512) \n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.scale_fc1 = torch.tensor(1.0) # quant-scale\n",
    "        self.range_fc1 = torch.tensor(1.0) # quant-range\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.scale_fc2 = torch.tensor(1.0) # quant-scale\n",
    "        self.range_fc2 = torch.tensor(1.0) # quant-range\n",
    "        self.output = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.bit = 8\n",
    "        self.x_bit = 2**(self.bit - 1) - 1\n",
    "        self.w_bit = 2**(self.bit - 1) - 1\n",
    "        self.y_bit = 2**(self.bit - 1) - 1\n",
    "        self.b_bit = 2**(self.bit*2 - 1) - 1\n",
    "        \n",
    "        self.Quantization = False\n",
    "        \n",
    "    def update_range(self, data, range_data):\n",
    "        now_range = data.abs().max()\n",
    "        if now_range > range_data:\n",
    "            return now_range\n",
    "        else:\n",
    "            return range_data\n",
    "        \n",
    "    def quantize(self, x, scale, int_range):\n",
    "        xq = torch.round(x * scale)\n",
    "        xq = torch.clip(xq, -int_range, int_range)\n",
    "        return xq\n",
    "        \n",
    "    def weight_quant(self, w, b, in_scale):\n",
    "        # scale of weight\n",
    "        scale_w = (2**(self.bit - 1) - 1) / w.abs().max()\n",
    "        # quantize weight\n",
    "        wq = self.quantize(w, scale_w, self.w_bit)\n",
    "        # scale of bias\n",
    "        scale_b = scale_w * in_scale\n",
    "        # quantize bais\n",
    "        bq = self.quantize(b, scale_b, self.b_bit)\n",
    "        \n",
    "        return wq, bq, scale_w\n",
    "        \n",
    "    def fusion_conv_bn_quant(self, conv, bn, in_scale):\n",
    "        # fusion conv and bn parameter\n",
    "        conv_w = conv.weight\n",
    "        conv_b = conv.bias\n",
    "        bn_gamma = bn.weight / torch.pow(bn.running_var + bn.eps, 0.5)\n",
    "        bn_beta = bn.bias - bn.running_mean * bn_gamma\n",
    "        fusion_w = conv_w * bn_gamma.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        fusion_b = conv_b * bn_gamma + bn_beta\n",
    "        \n",
    "        wq, bq, scale_w = self.weight_quant(fusion_w, fusion_b, in_scale)\n",
    "        \n",
    "        # update parameter\n",
    "        conv.weight.data = wq\n",
    "        conv.bias.data = bq\n",
    "        bn.running_var /= bn.running_var\n",
    "        bn.running_mean *= 0.0\n",
    "        bn.weight.data /= bn.weight\n",
    "        bn.bias.data *= 0.0\n",
    "        \n",
    "        return conv, bn, scale_w\n",
    "        \n",
    "    def cl_scale(self):\n",
    "        # Quantize activation tensor\n",
    "        self.scale_input = (2**(self.bit - 1) - 1) / self.range_input\n",
    "        self.scale_cv1 = (2**(self.bit - 1) - 1) / self.range_cv1\n",
    "        self.scale_cv2 = (2**(self.bit - 1) - 1) / self.range_cv2\n",
    "        self.scale_cv3 = (2**(self.bit - 1) - 1) / self.range_cv3\n",
    "        self.scale_cv4 = (2**(self.bit - 1) - 1) / self.range_cv4\n",
    "        self.scale_fc1 = (2**(self.bit - 1) - 1) / self.range_fc1\n",
    "        self.scale_fc2 = (2**(self.bit - 1) - 1) / self.range_fc2\n",
    "        \n",
    "        self.cnn1, self.bn1, self.cnn1_w_scale = self.fusion_conv_bn_quant(self.cnn1, self.bn1, self.scale_input)\n",
    "        self.cnn2, self.bn2, self.cnn2_w_scale = self.fusion_conv_bn_quant(self.cnn2, self.bn2, self.scale_cv1)\n",
    "        self.cnn3, self.bn3, self.cnn3_w_scale = self.fusion_conv_bn_quant(self.cnn3, self.bn3, self.scale_cv2)\n",
    "        self.cnn4, self.bn4, self.cnn4_w_scale = self.fusion_conv_bn_quant(self.cnn4, self.bn4, self.scale_cv3)\n",
    "        \n",
    "        self.fc1.weight.data, self.fc1.bias.data, self.fc1_w_scale = self.weight_quant(self.fc1.weight, self.fc1.bias, self.scale_cv4)\n",
    "        self.fc2.weight.data, self.fc2.bias.data, self.fc2_w_scale = self.weight_quant(self.fc2.weight, self.fc2.bias, self.scale_fc1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.Quantization:\n",
    "            self.range_input = self.update_range(x, self.range_input) # quant-觀察數據範圍\n",
    "            out = self.cnn1(x)\n",
    "            out = self.bn1(out)\n",
    "            out = self.relu1(out)\n",
    "            self.range_cv1 = self.update_range(out, self.range_cv1) # quant-觀察數據範圍\n",
    "            out = self.maxpool1(out)\n",
    "            out = self.cnn2(out)\n",
    "            out = self.bn2(out)\n",
    "            out = self.relu2(out)\n",
    "            self.range_cv2 = self.update_range(out, self.range_cv2) # quant-觀察數據範圍\n",
    "            out = self.maxpool2(out)\n",
    "            out = self.cnn3(out)\n",
    "            out = self.bn3(out)\n",
    "            out = self.relu3(out)\n",
    "            self.range_cv3 = self.update_range(out, self.range_cv3) # quant-觀察數據範圍\n",
    "            out = self.maxpool3(out)\n",
    "            out = self.cnn4(out)\n",
    "            out = self.bn4(out)\n",
    "            out = self.relu4(out)\n",
    "            self.range_cv4 = self.update_range(out, self.range_cv4) # quant-觀察數據範圍\n",
    "            out = self.maxpool4(out)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.fc1(out)\n",
    "            self.range_fc1 = self.update_range(out, self.range_fc1) # quant-觀察數據範圍\n",
    "            out = self.fc2(out)\n",
    "            self.range_fc2 = self.update_range(out, self.range_fc2) # quant-觀察數據範圍\n",
    "        else:\n",
    "            xq = self.quantize(x, self.scale_input, self.x_bit) # quant fp32 to int8\n",
    "            out = self.cnn1(xq)\n",
    "            out = self.relu1(out)\n",
    "            out = torch.clip(torch.round(out * self.scale_cv1 / (self.scale_input * self.cnn1_w_scale)), -self.x_bit, self.x_bit)\n",
    "            out = self.maxpool1(out)\n",
    "            out = self.cnn2(out)\n",
    "            out = self.relu2(out)\n",
    "            out = torch.clip(torch.round(out * self.scale_cv2 / (self.scale_cv1 * self.cnn2_w_scale)), -self.x_bit, self.x_bit)\n",
    "            out = self.maxpool2(out)\n",
    "            out = self.cnn3(out)\n",
    "            out = self.relu3(out)\n",
    "            out = torch.clip(torch.round(out * self.scale_cv3 / (self.scale_cv2 * self.cnn3_w_scale)), -self.x_bit, self.x_bit)\n",
    "            out = self.maxpool3(out)\n",
    "            out = self.cnn4(out)\n",
    "            out = self.relu4(out)\n",
    "            out = torch.clip(torch.round(out * self.scale_cv4 / (self.scale_cv3 * self.cnn4_w_scale)), -self.x_bit, self.x_bit)\n",
    "            out = self.maxpool4(out)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.fc1(out)\n",
    "            out = torch.clip(torch.round(out * self.scale_fc1 / (self.scale_cv4 * self.fc1_w_scale)), -self.x_bit, self.x_bit)\n",
    "            out = self.fc2(out)\n",
    "            out = torch.clip(torch.round(out * self.scale_fc2 / (self.scale_fc1 * self.fc2_w_scale)), -self.x_bit, self.x_bit)\n",
    "            \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloader<br />\n",
    "這邊就是建立dataloader的API，這裡就先不贅述。<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, datapath, datasize=(224, 224), mode='train'):\n",
    "        self.datapath = datapath\n",
    "        self.datasize = datasize\n",
    "        self.mode = mode\n",
    "        self.datanames = os.listdir(os.path.join(self.datapath, self.mode))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datanames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_path = os.path.join(self.datapath, self.mode, self.datanames[idx])\n",
    "        image = Image.open(image_path, mode='r')\n",
    "        image = image.convert('RGB')\n",
    "        image = image.resize(self.datasize)\n",
    "        image = np.array(image).transpose(2,0,1)\n",
    "        \n",
    "        if 'cat' in self.datanames[idx]:\n",
    "            label = [0]\n",
    "        else:\n",
    "            label = [1]\n",
    "        \n",
    "        image = torch.FloatTensor(image)\n",
    "        label = torch.LongTensor(label)\n",
    "        \n",
    "        return image, label\n",
    "     \n",
    "    def collate_fn(self, batch):\n",
    "        imgs = list()\n",
    "        labels = list()\n",
    "        for b in batch:\n",
    "            imgs.append(b[0])\n",
    "            labels.append(b[1])\n",
    "        imgs = torch.stack(imgs, dim=0)\n",
    "        labels = torch.stack(labels, dim=0)\n",
    "        return imgs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader setting<br />\n",
    "設定dataloader，依照代碼的需求，需要在根本代碼檔案相同路徑下建立一個cat_dog資料夾，裡面在建立一個val資料夾，放入圖片。<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 5\n",
    "train_dataset = dataset('cat_dog', datasize=(224, 224), mode='val')\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_dict:\n",
      " dict_keys(['cnn1.weight', 'cnn1.bias', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'cnn2.weight', 'cnn2.bias', 'bn2.weight', 'bn2.bias', 'bn2.running_mean', 'bn2.running_var', 'bn2.num_batches_tracked', 'cnn3.weight', 'cnn3.bias', 'bn3.weight', 'bn3.bias', 'bn3.running_mean', 'bn3.running_var', 'bn3.num_batches_tracked', 'cnn4.weight', 'cnn4.bias', 'bn4.weight', 'bn4.bias', 'bn4.running_mean', 'bn4.running_var', 'bn4.num_batches_tracked', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_Model(\n",
       "  (cnn1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn3): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn4): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): ReLU()\n",
       "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=968, out_features=512, bias=True)\n",
       "  (relu5): ReLU()\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (output): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_Model()\n",
    "\n",
    "# Load pre-train model\n",
    "checkpoint_backbone = torch.load('BEST_checkpoint.pth.tar', map_location=torch.device('cpu'))\n",
    "best_model = checkpoint_backbone['model']\n",
    "pretrained_dict = best_model.state_dict()\n",
    "model_dict = model.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "print(\"pretrained_dict:\\n\", pretrained_dict.keys())\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run float model inference<bn />\n",
    "先執行一遍把所有calibration資料都跑一遍，讓每層輸出都獲的該數據範圍。<bn />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 Top1: 100.000, 0/20\n",
      "\n",
      "FP32 Top1: 100.000, 1/20\n",
      "\n",
      "FP32 Top1: 100.000, 2/20\n",
      "\n",
      "FP32 Top1: 100.000, 3/20\n",
      "\n",
      "FP32 Top1: 100.000, 4/20\n",
      "\n",
      "FP32 Top1: 100.000, 5/20\n",
      "\n",
      "FP32 Top1: 100.000, 6/20\n",
      "\n",
      "FP32 Top1: 97.500, 7/20\n",
      "\n",
      "FP32 Top1: 97.778, 8/20\n",
      "\n",
      "FP32 Top1: 98.000, 9/20\n",
      "\n",
      "FP32 Top1: 98.182, 10/20\n",
      "\n",
      "FP32 Top1: 98.333, 11/20\n",
      "\n",
      "FP32 Top1: 98.462, 12/20\n",
      "\n",
      "FP32 Top1: 98.571, 13/20\n",
      "\n",
      "FP32 Top1: 97.333, 14/20\n",
      "\n",
      "FP32 Top1: 97.500, 15/20\n",
      "\n",
      "FP32 Top1: 97.647, 16/20\n",
      "\n",
      "FP32 Top1: 97.778, 17/20\n",
      "\n",
      "FP32 Top1: 96.842, 18/20\n",
      "\n",
      "FP32 Top1: 97.000, 19/20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classerr = tnt.ClassErrorMeter(accuracy=True, topk=(1, 2))\n",
    "\n",
    "end_i = train_loader.__len__()\n",
    "\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device).squeeze(-1)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    classerr.add(outputs.data, labels)\n",
    "\n",
    "    Top1 = classerr.value()[0]\n",
    "\n",
    "    print('FP32 Top1: {:.3f}, {}/{}\\n'.format(Top1, i, end_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run quantization inference<bn />\n",
    "計算scale，把Quantization功能打開，開始inference quantization mmodel。<bn />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 Top1: 97.143, 0/20\n",
      "\n",
      "INT8 Top1: 97.273, 1/20\n",
      "\n",
      "INT8 Top1: 97.391, 2/20\n",
      "\n",
      "INT8 Top1: 96.667, 3/20\n",
      "\n",
      "INT8 Top1: 96.800, 4/20\n",
      "\n",
      "INT8 Top1: 96.923, 5/20\n",
      "\n",
      "INT8 Top1: 97.037, 6/20\n",
      "\n",
      "INT8 Top1: 96.429, 7/20\n",
      "\n",
      "INT8 Top1: 95.862, 8/20\n",
      "\n",
      "INT8 Top1: 96.000, 9/20\n",
      "\n",
      "INT8 Top1: 96.129, 10/20\n",
      "\n",
      "INT8 Top1: 96.250, 11/20\n",
      "\n",
      "INT8 Top1: 96.364, 12/20\n",
      "\n",
      "INT8 Top1: 96.471, 13/20\n",
      "\n",
      "INT8 Top1: 96.000, 14/20\n",
      "\n",
      "INT8 Top1: 96.111, 15/20\n",
      "\n",
      "INT8 Top1: 96.216, 16/20\n",
      "\n",
      "INT8 Top1: 96.316, 17/20\n",
      "\n",
      "INT8 Top1: 96.410, 18/20\n",
      "\n",
      "INT8 Top1: 96.500, 19/20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.cl_scale()\n",
    "model.Quantization = True\n",
    "\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device).squeeze(-1)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    outputs /= model.fc2_w_scale\n",
    "    # print(outputs)\n",
    "\n",
    "    classerr.add(outputs.data, labels)\n",
    "\n",
    "    Top1 = classerr.value()[0]\n",
    "\n",
    "    print('INT8 Top1: {:.3f}, {}/{}\\n'.format(Top1, i, end_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
