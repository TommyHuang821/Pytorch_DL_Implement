{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of Convlution, BN, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(100)\n",
    "\n",
    "class conv_bn_relu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_bn_relu, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, stride=1, padding=1, groups=1, bias=True)\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting quantization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit = 8\n",
    "x_bit = 2**(bit - 1) - 1\n",
    "w_bit = 2**(bit - 1) - 1\n",
    "y_bit = 2**(bit - 1) - 1\n",
    "b_bit = 2**(bit*2 - 1) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Float model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = conv_bn_relu()\n",
    "model.eval()\n",
    "# create input tensor B*C*W*H\n",
    "xf = torch.rand(1, 1, 7, 7)\n",
    "# inference\n",
    "yf = model(xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Input quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale\n",
    "scale_x = (2**(bit - 1) - 1) / xf.abs().max()\n",
    "# quantize\n",
    "xq = torch.round(xf * scale_x)\n",
    "xq = torch.clip(xq, -x_bit, x_bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weight quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion conv and bn parameter\n",
    "conv = model.conv\n",
    "bn = model.bn\n",
    "conv_w = conv.weight\n",
    "conv_b = conv.bias\n",
    "bn_gamma = bn.weight / torch.pow(bn.running_var + bn.eps, 0.5)\n",
    "bn_beta = bn.bias - bn.running_mean * bn_gamma\n",
    "fusion_w = conv_w * bn_gamma.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "fusion_b = conv_b * bn_gamma + bn_beta\n",
    "\n",
    "# scale of weight\n",
    "scale_w = (2**(bit - 1) - 1) / fusion_w.abs().max()\n",
    "# quantize weight\n",
    "wq = torch.round(fusion_w * scale_w)\n",
    "wq = torch.clip(wq, -w_bit, w_bit)\n",
    "# scale of bias\n",
    "scale_b = scale_w * scale_x\n",
    "# quantize bais\n",
    "bq = torch.round(fusion_b * scale_b)\n",
    "bq = torch.clip(bq, -b_bit, b_bit)\n",
    "\n",
    "# update parameter\n",
    "model.conv.weight.data = wq\n",
    "model.conv.bias.data = bq\n",
    "model.bn.running_var /= model.bn.running_var\n",
    "model.bn.running_mean *= 0.0\n",
    "model.bn.weight.data /= model.bn.weight\n",
    "model.bn.bias.data *= 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Output quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale\n",
    "scale_y = (2**(bit - 1) - 1) / yf.abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quantization inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float inference:\n",
      " tensor([[[[0.0000, 0.0545, 0.0000, 0.0000, 0.0533],\n",
      "          [0.0000, 0.0000, 0.0000, 0.1838, 0.0000],\n",
      "          [0.0986, 0.0781, 0.1405, 0.0000, 0.0000],\n",
      "          [0.1124, 0.0923, 0.0378, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0463, 0.4461, 0.0000, 0.1777],\n",
      "          [0.3421, 0.0896, 0.1494, 0.0811, 0.3406],\n",
      "          [0.1011, 0.1305, 0.0357, 0.3610, 0.1571],\n",
      "          [0.2170, 0.0591, 0.1802, 0.0000, 0.0000],\n",
      "          [0.1733, 0.3323, 0.1169, 0.1065, 0.3219]],\n",
      "\n",
      "         [[0.4899, 0.5766, 0.2208, 0.2846, 0.4856],\n",
      "          [0.4484, 0.2111, 0.4620, 0.7209, 0.1935],\n",
      "          [0.2549, 0.3707, 0.6399, 0.5517, 0.3339],\n",
      "          [0.4240, 0.4600, 0.5220, 0.5862, 0.5554],\n",
      "          [0.5264, 0.5483, 0.5431, 0.5070, 0.2373]]]], grad_fn=<ReluBackward0>)\n",
      "Int8 inference:\n",
      " tensor([[[[0.0000, 0.0511, 0.0000, 0.0000, 0.0511],\n",
      "          [0.0000, 0.0000, 0.0000, 0.1816, 0.0000],\n",
      "          [0.1022, 0.0795, 0.1419, 0.0000, 0.0000],\n",
      "          [0.1135, 0.0965, 0.0397, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0454, 0.4484, 0.0000, 0.1760],\n",
      "          [0.3406, 0.0908, 0.1476, 0.0851, 0.3406],\n",
      "          [0.1022, 0.1249, 0.0341, 0.3633, 0.1589],\n",
      "          [0.2157, 0.0624, 0.1816, 0.0000, 0.0000],\n",
      "          [0.1760, 0.3292, 0.1135, 0.1078, 0.3179]],\n",
      "\n",
      "         [[0.4882, 0.5790, 0.2214, 0.2838, 0.4882],\n",
      "          [0.4484, 0.2100, 0.4655, 0.7209, 0.1930],\n",
      "          [0.2554, 0.3690, 0.6414, 0.5506, 0.3349],\n",
      "          [0.4257, 0.4598, 0.5222, 0.5847, 0.5563],\n",
      "          [0.5279, 0.5506, 0.5449, 0.5109, 0.2384]]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_pip = model(xq)\n",
    "yq = torch.round(y_pip * scale_y / (scale_x * scale_w))\n",
    "yq = torch.clip(yq, -y_bit, y_bit)\n",
    "yfq = yq / scale_y\n",
    "\n",
    "print('Float inference:\\n', yf)\n",
    "print('Int8 inference:\\n', yfq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
